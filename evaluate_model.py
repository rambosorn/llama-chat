import time
from rag_pipeline import ask_techcorp_ai
import statistics

# Test Questions covering different documents
TEST_QUESTIONS = [
    "What is the budget for Project Pegasus?",
    "What is the sick leave policy?",
    "How did the company perform in Q3?",
    "Who is the project manager for Pegasus?",
    "Can I work from home on Fridays?"
]

def run_evaluation():
    print("ğŸš€ Starting Model Evaluation...")
    print(f"ğŸ“‹ Running {len(TEST_QUESTIONS)} test queries...\n")
    
    results = []
    
    for i, question in enumerate(TEST_QUESTIONS, 1):
        print(f"[{i}/{len(TEST_QUESTIONS)}] Asking: {question}")
        
        # Run Pipeline
        response = ask_techcorp_ai(question)
        
        # Extract Metrics
        metrics = response.get('metrics', {})
        retrieval_t = metrics.get('retrieval_time', 0)
        generation_t = metrics.get('generation_time', 0)
        total_t = metrics.get('total_time', 0)
        
        results.append({
            "question": question,
            "answer": response.get('answer', "")[:100] + "...", # Truncate for display
            "retrieval": retrieval_t,
            "generation": generation_t,
            "total": total_t
        })
        
        print(f"   âœ… Done in {total_t}s (Ret: {retrieval_t}s | Gen: {generation_t}s)\n")

    # Calculate Averages
    avg_total = statistics.mean([r['total'] for r in results])
    avg_retrieval = statistics.mean([r['retrieval'] for r in results])
    avg_gen = statistics.mean([r['generation'] for r in results])
    
    # Generate Report
    report_content = f"""# ğŸ“Š AI Model Performance Report
**Date:** {time.strftime("%Y-%m-%d %H:%M:%S")}
**Model:** llama3.2

## âš¡ Summary Stats
| Metric | Average Time |
| :--- | :--- |
| **Total Response Time** | **{avg_total:.2f}s** |
| Retrieval Time | {avg_retrieval:.2f}s |
| Generation Time | {avg_gen:.2f}s |

## ğŸ“ Detailed Results
| Query | Retrieval (s) | Generation (s) | Total (s) |
| :--- | :--- | :--- | :--- |
"""
    
    for r in results:
        report_content += f"| {r['question']} | {r['retrieval']} | {r['generation']} | **{r['total']}** |\n"
        
    report_content += "\n*Generated by evaluate_model.py*"
    
    with open("performance_report.md", "w", encoding="utf-8") as f:
        f.write(report_content)
        
    print(f"ğŸ‰ Evaluation Complete! Report saved to 'performance_report.md'")
    print(f"ğŸ‘‰ Average Latency: {avg_total:.2f}s")

if __name__ == "__main__":
    run_evaluation()
